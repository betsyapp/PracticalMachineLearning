{"name":"Practicalmachinelearning","tagline":"","body":"# Practical Machine Learning – Final Project Write-Up\r\nFiles associated with this project are in my github repository here: https://github.com/betsyapp/PracticalMachineLearning.git\r\n\r\nThe goal of this project was to develop an algorithm to predict how well participants perform a particular physical activity using information about how the activity was executed. Specifically, the aim was to predict how well participants executed barbell lifts using data from accelerometers on the belt, forearm, arm, and dumbbell. There were five possible outcomes: the lift was performed exactly according to the specification (Class A), the participant threw his or her elbows to the front (Class B), the participant lifted the dumbbell only halfway (Class C), the participant lowered the dumbbell only halfway (Class D,) and and the participant threw his or her hips to the front (Class E).\r\n\r\n**Data Exploration & Cleaning**\r\nTwo sets of data were made available for this project: a training set consisting of 160 variables and 19,622 cases and a test set consisting of the same variables but only 20 cases.  (See http://groupware.les.inf.puc-rio.br/har for more information about the data and how they were obtained.)\r\n\r\nFirst, I read the training data and noted several missing values on many of the predictor variables.\r\n\r\n`#read and examine training and test data`\r\n`mydata <-read.csv(\"~/pml-training.csv\")`\r\n`dim(mydata)`\r\n`View(mydata)`\r\n\r\nI removed variables that had one or more missing values in the training set, and I removed variables that were neither physical parameters to use for prediction nor the outcome variable I wanted to predict (the “classe” of the activity—A, B, C, D, or E). I performed the same manipulations on the test set so it would mirror the training set.\r\n\r\n`mydata <- mydata[c(2:11,37:49,60:68,84:86,102,113:124,140,151:160)]`\r\n`testdata <- testdata[c(2:11,37:49,60:68,84:86,102,113:124,140,151:160)]`\r\n`mydata <- mydata[-c(1:6)]`\r\n`testdata <- testdata[-c(1:6)]`\r\n\r\nI reviewed the structure of the training data and determined I could move forward with developing a prediction algorithm.\r\n\r\n`str(mydata)`\r\n\r\nMy final training and testing sets comprised 52 predictors and 1 outcome variable (“classe”)\r\n\r\n**Data Partitioning**\r\nNext, I separated the training data into two parts: a training set and a hold-out sample to test the algorithm built using the training set. I randomly assigned three-quarters of the training data to be the training set and the remaining quarter to be the test set.\r\n\r\n`#create training and testing data frames`\r\n`set.seed(814)`\r\n`inTrain <- createDataPartition(y=mydata$classe,p=.75,list=FALSE)`\r\n`training <- mydata[inTrain,]`\r\n`testing <- mydata[-inTrain,]`\r\n\r\nThe training set comprised 14,718 cases while the test set comprised 4,904.\r\n\r\n**Model Construction & Results**\r\nIn my first attempt, I used linear discriminant analysis with 5-fold cross-validation in my first attempt to develop an algorithm that would accurately classify cases into the five possible “classe” categories. I used LDA because this is an analytic approach with which my colleagues are familiar.\r\n\r\n`#fit linear discriminant analysis`\r\n`library(caret)`\r\n`mycontrol <- trainControl(method=\"cv\", number=5, allowParallel=TRUE, verbose=TRUE)`\r\n`modFitlda <- train(classe ~ .,method=\"lda\",data=training, trControl=mycontrol, verbose=FALSE)`\r\n\r\nThe train function was executed very quickly. However, accuracy suffered substantially.\r\n\r\n\t`modFitlda`\r\n`Linear Discriminant Analysis `\r\n\r\n`14718 samples`\r\n   `52 predictor`\r\n    `5 classes: 'A', 'B', 'C', 'D', 'E' `\r\n\r\n`No pre-processing`\r\n`Resampling: Cross-Validated (5 fold) `\r\n`Summary of sample sizes: 11775, 11773, 11776, 11772, 11776 `\r\n`Resampling results`\r\n\r\n  `Accuracy   Kappa     Accuracy SD  Kappa SD   `\r\n  `0.6996201  0.619922  0.005049872  0.006399358`\r\n\r\n\r\nWithin the training set, accuracy was less than 70%. When applied to the test set, accuracy was even lower at 71% (see confusion matrix below).\r\n\r\npredlda    A    B    C    D    E\r\n\r\nA 1158  132   84   61   30\r\n\r\nB   37  620   74   28  159\r\n\r\nC   97  116  574   84   79\r\n\r\nD   97   32   99  603   98\r\n\r\nE    6   49   24   28  535\r\n\r\n\r\n`Due to low accuracy, I decided to implement random forest instead, again using 5-fold cross-validation in an effort to avoid overfitting.`\r\n\r\n`modFitrf <- train(classe ~ .,method=\"rf\",data=training,trControl=mycontrol, verbose=FALSE)`\r\n`Random Forest `\r\n\r\n`14718 samples`\r\n   `52 predictor`\r\n    `5 classes: 'A', 'B', 'C', 'D', 'E' `\r\n\r\n`No pre-processing`\r\n`Resampling: Cross-Validated (5 fold) `\r\n`Summary of sample sizes: 11775, 11773, 11776, 11772, 11776 `\r\n`Resampling results across tuning parameters:`\r\n\r\n  `mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   `\r\n   `2    0.9924571  0.9904581  0.002573112  0.003255982`\r\n  `27    0.9921179  0.9900289  0.002249167  0.002845970`\r\n  `52    0.9845767  0.9804874  0.002004701  0.002535956`\r\n\r\n`Accuracy was used to select the optimal model using  the largest value.`\r\n`The final value used for the model was mtry = 2. `\r\n\r\n\r\nThis approach proved much more accurate than my initial attempt with LDA, with zero classification errors in training.\r\n\r\npredrf    A    B    C    D    E\r\n\r\nA 4185    0    0    0    0\r\n\r\nB    0 2848    0    0    0\r\n\r\nC    0    0 2567    0    0\r\n\r\nD    0    0    0 2412    0\r\n\r\nE    0    0    0    0 270\r\n\r\n\r\nWhen I applied the model to the test set, accuracy was still extremely high at 99%.\r\n\r\npredrf    A    B    C    D    E\r\n\r\nA 1395    5    0    0    0\r\n\r\nB    0  942    7    0    0\r\n\r\nC    0    2  848   15    1\r\n\r\nD    0    0    0  788    3\r\n\r\nE    0    0    0    1  897\r\n\r\nThe confusion matrix shows there were only 32 classification errors of the total 4,904 cases in the test set.\r\nIn sum, the final model built using random forest with 5-fold cross-validation was highly accurate with an expected out-of-sample error rate of less than 5%, which justified the cost in terms of speed.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}